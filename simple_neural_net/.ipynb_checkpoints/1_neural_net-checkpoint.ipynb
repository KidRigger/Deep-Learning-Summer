{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Net implementation using numpy\n",
    "#### Using sigmoid activation and gradient descent\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Only need numpy and pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to one hot encode the output for being able to train the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(T,nclasses):\n",
    "    \"\"\"One hot encodes the given T column vector made for MNIST\"\"\"\n",
    "    y = np.zeros((T.shape[0],n_classes))\n",
    "    for i,d in enumerate(T):\n",
    "        y[i,d-1] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An initialization for the Multilayer Neural Net in the system.\n",
    "Takes parameters for just the layer size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultilayerNN:\n",
    "    \"\"\" Multilayer Neural Net\"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layers = None):\n",
    "        \"\"\"Initializes a neural net with random weights of given input size, \n",
    "        output size and the mentioned hidden layers\"\"\"\n",
    "        self.w = []\n",
    "        if hidden_layers == None or len(hidden_layers) == 0:\n",
    "            self.w.append(self._gen_layer_weights(input_size+1,output_size))\n",
    "        layers = len(hidden_layers)\n",
    "        if layers == 1:\n",
    "            self.w.append(self._gen_layer_weights(input_size+1,hidden_layers[0]))\n",
    "            self.w.append(self._gen_layer_weights(hidden_layers[-1]+1,output_size))\n",
    "        else:\n",
    "            self.w.append(self._gen_layer_weights(input_size+1,hidden_layers[0]))\n",
    "            for i in range(1,layers):\n",
    "                self.w.append(self._gen_layer_weights(hidden_layers[i-1]+1,hidden_layers[i]))\n",
    "            self.w.append(self._gen_layer_weights(hidden_layers[-1]+1,output_size))\n",
    "        self.nlayers = len(self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates random weights for the layer\n",
    "Just a simple random generation $\\sigma(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _gen_layer_weights(m, n):\n",
    "    \"\"\"Generates uniform distribution of weights with mean 0 and variance 1 for layer of shape (m,n)\"\"\"\n",
    "    return np.random.normal(0,1,(m,n))\n",
    "\n",
    "MultilayerNN._gen_layer_weights = _gen_layer_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function with a derivative\n",
    "Sigmoid = $\\frac{1}{1-e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sigmoid(self,X, deriv=False):\n",
    "    \"\"\"Calculates the sigmoid of the given X, \n",
    "    if deriv is true, returns the derivative of sigmoid\"\"\"\n",
    "    if deriv:\n",
    "        return self._sigmoid(X)*self._sigmoid(1-X)\n",
    "    return (1/(1+np.exp(-X)))\n",
    "\n",
    "MultilayerNN._sigmoid = _sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the Neural Net to the dataset\n",
    "Usign gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(self,X,Y,LR,epochs):\n",
    "    \"\"\"Fits the neural net to the given data X and Y with learning rate LR for given epochs\"\"\"\n",
    "    j = np.zeros((epochs,1))\n",
    "    m = Y.shape[0]\n",
    "    for i in range(epochs):\n",
    "        out = self._train_epoch(X,Y,LR)\n",
    "        j[i] = self._cost(out,Y)\n",
    "        acc = np.sum(np.argmax(out,axis=1) == np.argmax(Y,axis=1))/m\n",
    "        print(\"%d - Acc: %f Cost: %f\"%(i,acc,j[i]))\n",
    "    return j\n",
    "\n",
    "MultilayerNN.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_epoch(self,X,Y,LR):\n",
    "    \"\"\"Carries out one training epoch of the neural network.\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    layers = [None]*self.nlayers\n",
    "    lin = X\n",
    "    for i,t in enumerate(self.w):\n",
    "        lin2 = np.column_stack((np.ones((lin.shape[0],1)),lin))\n",
    "        out = self._sigmoid(lin2@t)\n",
    "        layers[i] = (lin,out,lin2)\n",
    "        lin = out\n",
    "    \n",
    "    grad = [None]*self.nlayers\n",
    "    \n",
    "    # Output layer\n",
    "    delta_o = (Y - layers[-1][1])*self._sigmoid(layers[-1][1],deriv=True)\n",
    "    grad[-1] = (1/m)*(layers[-1][2].transpose()@delta_o)\n",
    "\n",
    "    for i in range(2,len(layers)+1):\n",
    "        delta_o = (delta_o@self.w[1-i].transpose())[:,1:]*self._sigmoid(layers[-i][1],deriv=True)\n",
    "        grad[-i] = (1/m)*(layers[-i][2].transpose()@delta_o)\n",
    "        \n",
    "    for i in range(len(layers)):\n",
    "        self.w[i] += LR*grad[i]\n",
    "        #print(grad[i])\n",
    "    \n",
    "    return out\n",
    "        \n",
    "MultilayerNN._train_epoch = _train_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "Cost function be cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _cost(self, out, Y):\n",
    "    \"\"\"Calculates cost of the neural nets prediction\"\"\"\n",
    "    return np.sum(-Y*np.log(out) - (1-Y)*np.log(1-out))\n",
    "\n",
    "MultilayerNN._cost = _cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(self,X):\n",
    "    \"\"\"Predicts output as per the weights for the given output.\"\"\"\n",
    "    lin = X\n",
    "    for t in self.w:\n",
    "        lin2 = np.column_stack((np.ones((lin.shape[0],1)),lin))\n",
    "        lin = self._sigmoid(lin2@t)\n",
    "    return lin\n",
    "\n",
    "MultilayerNN.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading weights\n",
    "Using pickle to load and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(self,filename):\n",
    "    \"\"\"Saves the trained weights as a pickle binary file.\"\"\"\n",
    "    with open(filename,'wb') as f:\n",
    "        pkl.dump(self.w,f)\n",
    "    \n",
    "def load(self,filename):\n",
    "    \"\"\"Loads the trained weights from the pickle file.\"\"\"\n",
    "    with open(filename,'rb') as f: \n",
    "        self.w = pkl.load(f)\n",
    "    \n",
    "MultilayerNN.save = save\n",
    "MultilayerNN.load = load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
